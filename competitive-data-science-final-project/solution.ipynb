{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you have to install ipython-autotime using 'pip install ipython-autotime'\n",
    "%load_ext autotime\n",
    "\n",
    "import gc\n",
    "import IPython.display\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# in this project, the metric is rmse, not mse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# SVR and KNeighborsRegressor is too slow to apply this data\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#TODO: sklearn AdaBoost, GradientBoosting 사용\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "seed = 180718\n",
    "\n",
    "# lag 양을 여기서 결정\n",
    "shift_range = [1,2,3,12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "sales = pd.read_csv('./dataset/sales_train.csv.gz')\n",
    "shops = pd.read_csv('./dataset/shops.csv')\n",
    "items = pd.read_csv('./dataset/items.csv')\n",
    "#item_cats = pd.read_csv('./dataset/item_categories.csv')\n",
    "test = pd.read_csv(\"./dataset/test.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make utilities to submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function makes codes simple, so it's good to make these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.98 ms\n"
     ]
    }
   ],
   "source": [
    "def make_submission_df(all_prediction):\n",
    "    df = test.merge(all_prediction, on=[\"shop_id\", \"item_id\"], how=\"left\")[[\"ID\", \"item_cnt_month\"]]\n",
    "    df[\"item_cnt_month\"] = df[\"item_cnt_month\"].fillna(0).clip(0, 20)    \n",
    "    return df\n",
    "\n",
    "def make_submission_file(df, filename):\n",
    "    df.to_csv(\"./submission/%s.csv\" % filename, index=False)\n",
    "    \n",
    "def make_submission(all_prediction, filename=\"no_name\"):\n",
    "    make_submission_file(make_submission_df(all_prediction), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be benchmarks to measure my prediction's quality, so I made very simple ones. I think it should be done in first phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = pd.read_csv('./dataset/sample_submission.csv.gz')\n",
    "# make_submission_file(sample, 'sample_value')\n",
    "\n",
    "# sample['item_cnt_month'] = 0\n",
    "# make_submission_file(sample, 'zero_value')\n",
    "\n",
    "# previous_month = sales[sales[\"date_block_num\"] == 33].groupby([\"shop_id\", \"item_id\"], as_index=False).item_cnt_day.sum().rename(columns={\"item_cnt_day\": \"item_cnt_month\"})\n",
    "# make_submission(previous_month, \"previous_month_value\")\n",
    "\n",
    "# del sample, previous_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sample value(all 0.5): 1.23646\n",
    "* zero value: 1.25011\n",
    "* previous month value: 1.16777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze raw datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start to anylyze basic information about give datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make float data looks integer data\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "sales.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do simple calculations here. The number of shop_id is 60, and the number of item_id is 22,170. Therefore, the total number of combinations of them is 1,330,200. However, there are only 214,200 IDs in the test. It means that this competition only requires 16.1% of the all shop_id and item_id combinations.\n",
    "\n",
    "We can use this fact in 3 ways.\n",
    "1. get a prediction of the test IDs in the submission using full data in the training and the validation.\n",
    "2. get a prediction of the test IDs in the validation and the submission using full data in the training.\n",
    "3. Reduce data before training to make training short.\n",
    "\n",
    "I think we should take 2 or 3. In the first way, the validation score can not be fitting to the test score. My strategy is using 3 till the validation and using 2 in the submission only. I think full data has other shops or other items, but it can give some information about how the price is going especially if I use RNN algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan EDA(Exploratory data analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think item_price and item_cnt_day have interesting qualtiles and min-max values. First of all, item_cnt_day must not be zero value because sales data is record of something that occured in sales. However, the target is item_cnt_month, so it would be better to analyze monthly data of item_cnt. In the item, the max price is so much higher than others. I'm not sure but, it's possible to use the extream price for prediction.\n",
    "\n",
    "My plans is as below.\n",
    "\n",
    "1. reduce data using test id combinations\n",
    "2. aggregate the total item_cnt_month of shops month by month\n",
    "3. aggregate the total item_cnt month of items month by month\n",
    "4. aggregate the total item_cnt_month month by month\n",
    "\n",
    "The purpose of them is to know if there are correlations between them and if there are patterns in time flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce data using test id combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_sales = sales.merge(test)\n",
    "reduced_sales = reduced_sales.drop('ID', axis=1)\n",
    "reduced_sales.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before reducing data, the total number of rows is 2,935,849. Now, the amount of data is reduced to 41.7%.\n",
    "It means that test data is not randomly picked in all combinations of shop_id and item_id.\n",
    "One of the possible scenarios is that the host of this competition chose test targets in combinations that appeared in the sales data, not in all combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze combinations of shop_id and item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_comb = sales[['shop_id', 'item_id']]\n",
    "full_comb = full_comb.drop_duplicates()\n",
    "display(full_comb.describe())\n",
    "display('unique value of shop_id: ' + str(len(full_comb.shop_id.unique())))\n",
    "display('unique value of item_id: ' + str(len(full_comb.item_id.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_comb = reduced_sales[['shop_id', 'item_id']]\n",
    "reduced_comb = reduced_comb.drop_duplicates()\n",
    "display(reduced_comb.describe())\n",
    "display('unique value of shop_id: ' + str(len(reduced_comb.shop_id.unique())))\n",
    "display('unique value of item_id: ' + str(len(reduced_comb.item_id.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(test.describe())\n",
    "display('unique value of shop_id: ' + str(len(test.shop_id.unique())))\n",
    "display('unique value of item_id: ' + str(len(test.item_id.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| data | full | reduced | test |\n",
    "|------|------| ------- | ---- |\n",
    "| shop_id | 60 | 42 | 42 |\n",
    "| item_id | 21,807 | 4,716 | 5100 |\n",
    "| total | 424,124 | 111,404 | 214,200 |\n",
    "| possible | 1,308,420 | 198,072 | 214,200 |\n",
    "| ratio | 32.4% | 56.2% | 100% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target is 214,200 combination. However, in the reduced data, there is only 4,716 unique item_ids. It means that 385 item was not sold in that period. In the combination, there is more zero sold combinations. It's almost half of the test combinations. In the full data set, zero sold combination ratios is abount 1/3. This is not so big gap between them. I think the important is the number of item_id. The test unique item_id is almost 1/4 of full data item_id. If we use one hot encoding for item_id, we can use only 1/4 of memory.\n",
    "\n",
    "I focused on something else. The total number of the combinations in the test is 214,200, but 111,404 in the reduced dataset. It means that only about half of combinations exists in sales data. One more data selection options is selecting only data in 111,404 combinations. I'm going to use the test dataset first, and then I'll use smaller and bigger one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del full_comb, reduced_comb, reduced_sales\n",
    "pd.options.display.float_format = '{:,.5f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get applicable dataset to models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a useful function to save a memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.98 ms\n"
     ]
    }
   ],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get base data form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The form should have 'shop_id', 'item_id', 'date_block_num' because the required form of this competition is 'ID' made of 'shop_id' and 'item_id', and 'item_cnt_month'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "gb = sales.groupby(index_cols, as_index=False).sum().rename(columns={'item_cnt_day':'month_sale'})\n",
    "\n",
    "del sales\n",
    "\n",
    "# 가격 정보를 사용할 것인지 아직 모름. 사용한다면 살려야 함.\n",
    "gb = gb.drop('item_price', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.88 s\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({'shop_id': np.sort(shops.shop_id.unique()), 'key':np.zeros(len(shops.shop_id.unique()))})\n",
    "df2 = pd.DataFrame({'item_id': np.sort(items.item_id.unique()), 'key':np.zeros(len(items.item_id.unique()))})\n",
    "df3 = pd.DataFrame({'date_block_num': np.sort(gb.date_block_num.unique()), 'key':np.zeros(len(gb.date_block_num.unique()))})\n",
    "\n",
    "df = df1.merge(df2).merge(df3)\n",
    "\n",
    "del df1, df2, df3, shops\n",
    "\n",
    "df = df.drop('key', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "df = df.merge(gb, how='outer').fillna(0)\n",
    "\n",
    "del gb\n",
    "\n",
    "df.head()\n",
    "\n",
    "df = downcast_dtypes(df)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip (0, 20) before making something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>month_sale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  date_block_num  month_sale\n",
       "0        0        0               0         0.0\n",
       "1        0        0               1         0.0\n",
       "2        0        0               2         0.0\n",
       "3        0        0               3         0.0\n",
       "4        0        0               4         0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 250 ms\n"
     ]
    }
   ],
   "source": [
    "df.month_sale = df.month_sale.values.clip(0,20)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# montly sale in the shop, item, category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "shop_gb_sum = df.groupby(['shop_id', 'date_block_num'], as_index=False).sum()\n",
    "shop_gb_sum = shop_gb_sum.rename(columns={'month_sale':'month_sale_shop_sum'})\n",
    "shop_gb_sum = shop_gb_sum.drop(columns=['item_id'])\n",
    "\n",
    "shop_gb_mean = df.groupby(['shop_id', 'date_block_num'], as_index=False).mean()\n",
    "shop_gb_mean = shop_gb_mean.rename(columns={'month_sale':'month_sale_shop_mean'})\n",
    "shop_gb_mean = shop_gb_mean.drop(columns=['item_id'])\n",
    "\n",
    "item_gb_sum = df.groupby(['item_id', 'date_block_num'], as_index=False).sum()\n",
    "item_gb_sum = item_gb_sum.rename(columns={'month_sale':'month_sale_item_sum'})\n",
    "item_gb_sum = item_gb_sum.drop(columns=['shop_id'])\n",
    "\n",
    "item_gb_mean = df.groupby(['item_id', 'date_block_num'], as_index=False).mean()\n",
    "item_gb_mean = item_gb_mean.rename(columns={'month_sale':'month_sale_item_mean'})\n",
    "item_gb_mean = item_gb_mean.drop(columns=['shop_id'])\n",
    "\n",
    "category = items.drop(columns=['item_name'], axis=1)\n",
    "category = category.rename(columns={'item_category_id':'category_id'})\n",
    "\n",
    "item_sum_with_cat_gb = item_gb_sum.merge(category, how='left')\n",
    "\n",
    "category_gb_sum = item_sum_with_cat_gb.groupby(['category_id', 'date_block_num'], as_index=False).sum()\n",
    "category_gb_sum = category_gb_sum.rename(columns={'month_sale_item_sum':'month_sale_category_sum'})\n",
    "category_gb_sum = category_gb_sum.drop(columns=['item_id'], axis=1)\n",
    "\n",
    "category_gb_mean = item_sum_with_cat_gb.groupby(['category_id', 'date_block_num'], as_index=False).mean()\n",
    "category_gb_mean = category_gb_mean.rename(columns={'month_sale_item_sum':'month_sale_category_mean'})\n",
    "category_gb_mean = category_gb_mean.drop(columns=['item_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "df = df.merge(shop_gb_sum, how='left').fillna(0)\n",
    "df = df.merge(shop_gb_mean, how='left').fillna(0)\n",
    "\n",
    "df = df.merge(item_gb_sum, how='left').fillna(0)\n",
    "df = df.merge(item_gb_mean, how='left').fillna(0)\n",
    "\n",
    "df = df.merge(category, how='left').fillna(0)\n",
    "df = df.merge(category_gb_sum, how='left').fillna(0)\n",
    "df = df.merge(category_gb_mean, how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 112 ms\n"
     ]
    }
   ],
   "source": [
    "del shop_gb_sum, shop_gb_mean, item_gb_sum, item_gb_mean, category_gb_sum, category_gb_mean, category, item_sum_with_cat_gb, items\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# montly sale in the item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "#12개월 전체를 할 수도 있고 일부를 할 수도 있다. 난 여기서 일부만 사용\n",
    "#ensembling에선 1~5,12를 사용\n",
    "\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num', 'category_id']\n",
    "\n",
    "cols_to_rename = list(df.columns.difference(index_cols)) \n",
    "df = downcast_dtypes(df)\n",
    "lag_df = df.copy()\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe4e028e05243c6a65779043ab57e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    train_shift = lag_df[index_cols + cols_to_rename].copy()\n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    lag_df = lag_df.merge(train_shift, how='outer')\n",
    "    \n",
    "    del train_shift, foo\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desca\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:2540: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "# 이유는 모르겠는데 끝나고 나면 lag_df에서 index column들이 64비트로 변경됨\n",
    "lag_df[index_cols] = downcast_dtypes(lag_df[index_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Abort unnecessary data in lag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 53.4 s\n"
     ]
    }
   ],
   "source": [
    "lag_df = lag_df[12 <= lag_df.date_block_num]\n",
    "lag_df = lag_df[lag_df.date_block_num <= 34]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save and Load lag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17 ms\n"
     ]
    }
   ],
   "source": [
    "#압축하면서 저장하는게 너무 오래걸린다. 나중에 시간 있을 때 한번 돌려보자.\n",
    "#lag_df.to_csv(\"lag_df.csv.gz\", index=False, compression='gzip')\n",
    "\n",
    "# lag_df = pd.read_csv(\"lag_df.csv.gz\")\n",
    "# lag_df = downcast_dtypes(lag_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train / valid / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 834 ms\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "dates = lag_df['date_block_num']\n",
    "fit_cols = [col for col in lag_df.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "to_drop_cols = list(set(list(lag_df.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.91 s\n"
     ]
    }
   ],
   "source": [
    "valid_block = 33\n",
    "\n",
    "dates_train = dates[dates <  valid_block]\n",
    "dates_valid  = dates[dates == valid_block]\n",
    "\n",
    "X_train = lag_df.loc[dates <  valid_block].drop(to_drop_cols, axis=1)\n",
    "y_train = lag_df.loc[dates <  valid_block, 'month_sale'].values.clip(0,20)\n",
    "\n",
    "valid = lag_df.loc[dates == valid_block]\n",
    "valid = test.merge(valid, how='left').fillna(0).drop('ID', axis=1)\n",
    "X_valid =  valid.drop(to_drop_cols, axis=1)\n",
    "y_valid =  valid['month_sale'].values.clip(0,20)\n",
    "\n",
    "del valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40 s\n"
     ]
    }
   ],
   "source": [
    "test_block = 34\n",
    "\n",
    "dates_full_train = dates[dates < test_block]\n",
    "dates_test = dates[dates == test_block]\n",
    "\n",
    "X_full_train = lag_df.loc[dates <  test_block].drop(to_drop_cols, axis=1)\n",
    "y_full_train = lag_df.loc[dates <  test_block, 'month_sale'].values.clip(0,20)\n",
    "X_test = lag_df.loc[dates == test_block].drop(to_drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define this competition metric as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.9 ms\n"
     ]
    }
   ],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.clip(y_true, 0, 20), np.clip(y_pred, 0, 20)))\n",
    "\n",
    "def get_valid_rmse(reg):\n",
    "    reg.fit(X_train.values, y_train)\n",
    "    pred = reg.predict(X_valid.values)\n",
    "    return rmse(pred, y_valid)\n",
    "\n",
    "def get_prediction(reg):\n",
    "    reg.fit(X_full_train.values, y_full_train)\n",
    "    pred = reg.predict(X_test.values)\n",
    "    return pred\n",
    "\n",
    "def make_submission_from_prediction(pred, filename):\n",
    "    sub = lag_df[dates == test_block]\n",
    "    sub = sub.rename(columns={'month_sale':'item_cnt_month'})\n",
    "    sub = sub[['shop_id', 'item_id', 'month_sale']]\n",
    "    make_submission(sub, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First level models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8998342661018064"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 58.1 s\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(n_jobs=4)\n",
    "get_valid_rmse(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9970715380115442"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6min 50s\n"
     ]
    }
   ],
   "source": [
    "enet = ElasticNet(n_jobs=4)\n",
    "get_valid_rmse(enet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8867281599288455"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "lgb = LGBMRegressor(n_jobs=4, random_state=seed)\n",
    "get_valid_rmse(lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8821269853328018"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40min 15s\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=20, max_depth=13, n_jobs=4, random_state=seed)\n",
    "get_valid_rmse(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shop_id: 0.005938857500999187\n",
      "item_id: 0.021326064336808163\n",
      "category_id: 0.007972726863236742\n",
      "month_sale_lag_1: 0.6465714500061555\n",
      "month_sale_category_mean_lag_1: 0.008624141447710783\n",
      "month_sale_category_sum_lag_1: 0.007536008250074994\n",
      "month_sale_item_mean_lag_1: 0.039725975588308204\n",
      "month_sale_item_sum_lag_1: 0.03251625320541881\n",
      "month_sale_shop_mean_lag_1: 0.006472418298511968\n",
      "month_sale_shop_sum_lag_1: 0.0070434549731058504\n",
      "month_sale_lag_2: 0.03364581297228696\n",
      "month_sale_category_mean_lag_2: 0.008653950221609997\n",
      "month_sale_category_sum_lag_2: 0.008892765429332431\n",
      "month_sale_item_mean_lag_2: 0.008119195875594955\n",
      "month_sale_item_sum_lag_2: 0.007996687915974625\n",
      "month_sale_shop_mean_lag_2: 0.006869937708972531\n",
      "month_sale_shop_sum_lag_2: 0.0058250433257976495\n",
      "month_sale_lag_3: 0.04117458427741591\n",
      "month_sale_category_mean_lag_3: 0.00802149236251935\n",
      "month_sale_category_sum_lag_3: 0.010712345698485195\n",
      "month_sale_item_mean_lag_3: 0.004117669543735631\n",
      "month_sale_item_sum_lag_3: 0.0038548439009915684\n",
      "month_sale_shop_mean_lag_3: 0.0038313732530357893\n",
      "month_sale_shop_sum_lag_3: 0.0035420654092734065\n",
      "month_sale_lag_12: 0.004775491533169776\n",
      "month_sale_category_mean_lag_12: 0.013202608879856751\n",
      "month_sale_category_sum_lag_12: 0.007057560002099656\n",
      "month_sale_item_mean_lag_12: 0.0020956732202855473\n",
      "month_sale_item_sum_lag_12: 0.0023415666860874713\n",
      "month_sale_shop_mean_lag_12: 0.014805446457966016\n",
      "month_sale_shop_sum_lag_12: 0.016736534855178606\n",
      "time: 3.24 s\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_train.columns.values)):\n",
    "    print(str(X_train.columns.values[i]) + \": \" + str(rf.feature_importances_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87428755"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 36min 21s\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBRegressor(max_depth=10, learning_rate=0.03, n_jobs=4, random_state=seed)\n",
    "get_valid_rmse(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-230871df38bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmake_submission_from_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rf_n20_m13'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-0f7200592133>\u001b[0m in \u001b[0;36mget_prediction\u001b[1;34m(reg)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_full_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_full_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \"\"\"\n\u001b[0;32m    246\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                       force_all_finite)\n\u001b[0;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "make_submission_from_prediction(get_prediction(rf), 'rf_n20_m13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission_from_prediction(get_prediction(xgb), 'xgb_m10_l3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_params = {\n",
    "#                'feature_fraction': 0.75,\n",
    "#                'metric': 'rmse',\n",
    "#                'nthread':1, \n",
    "#                'min_data_in_leaf': 2**7, \n",
    "#                'bagging_fraction': 0.75, \n",
    "#                'learning_rate': 0.03, \n",
    "#                'objective': 'mse', \n",
    "#                'bagging_seed': 2**7, \n",
    "#                'num_leaves': 2**7,\n",
    "#                'bagging_freq':1,\n",
    "#                'verbose':0 \n",
    "#               }\n",
    "\n",
    "# lgb = lightgbm.train(lgb_params, lightgbm.Dataset(X_train.values, label=y_train), 100)\n",
    "# pred_lgb = lgb.predict(X_valid)\n",
    "# rmse(pred_lgb, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple ensembling structure - data는.. cat전\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ensemble two model\n",
    "    1. Linear Regression(0.90268626150226106), lightgbm(0.87875613368405925) =>\n",
    "        1. last 3\n",
    "            1. Linear Regression: 0.879674\n",
    "            2. lightgbm: 0.947700\n",
    "            3. Random Forest: 0.881135\n",
    "        2. last 6\n",
    "            1. Linear Regression: 0.877625\n",
    "            2. lightgbm: 0.959168\n",
    "            3. Random Forest: 0.878719\n",
    "        3. last 6 + X_train\n",
    "            1. Linear Regression: 0.876767 => 1.00935\n",
    "            2. lightgbm: 0.882532\n",
    "            3. **Random Forest: 0.877934 => 1.00924**\n",
    "            4. ElasticNet(alpha=0.01): 0.888654\n",
    "            5. xgb: ??? => 1.01291\n",
    "- ensemble three model\n",
    "    1. Linear Regression, lightgbm, KnnRegressor\n",
    "        - winner in two models\n",
    "- ensemble five models\n",
    "    1. Linear Regression, ElasticNet, lightgbm, Random Forest, KnnRegressor\n",
    "        - ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_level2 = np.c_[pred_lr, pred_lgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#level2_date_blocks = [27,28,29,30,31,32]\n",
    "# dates_train_level2 = dates_train[dates_train.isin(level2_date_blocks)]\n",
    "#y_train_level2 = y_train[dates_train.isin(level2_date_blocks)]\n",
    "\n",
    "\n",
    "# for prediction\n",
    "level2_date_blocks = [28,29,30,31,32,33]\n",
    "dates_train_level2 = dates_full_train[dates_full_train.isin(level2_date_blocks)]\n",
    "y_train_level2 = y_full_train[dates_full_train.isin(level2_date_blocks)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# And here we create 2nd level feeature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n",
    "\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in tqdm_notebook(level2_date_blocks): \n",
    "    \n",
    "     gc.collect()\n",
    "#     lr = LinearRegression()\n",
    "#     lr.fit(X_train[dates_train < cur_block_num].values, y_train[dates_train < cur_block_num])\n",
    "#     pred_lr_level2 = lr.predict(X_train[dates_train == cur_block_num].values)\n",
    "    \n",
    "#     lgb = lightgbm.train(lgb_params, lightgbm.Dataset(X_train[dates_train < cur_block_num], label=y_train[dates_train < cur_block_num]), 100)\n",
    "#     pred_lgb_level2 = lgb.predict(X_train[dates_train == cur_block_num].values)\n",
    "    \n",
    "#     X_train_level2[dates_train_level2.isin([cur_block_num]), 0] = pred_lr_level2.copy()\n",
    "#     X_train_level2[dates_train_level2.isin([cur_block_num]), 1] = pred_lgb_level2.copy()\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_full_train[dates_full_train < cur_block_num].values, y_full_train[dates_full_train < cur_block_num])\n",
    "    pred_lr_level2 = lr.predict(X_full_train[dates_full_train == cur_block_num].values)\n",
    "    \n",
    "    lgb = lightgbm.train(lgb_params, lightgbm.Dataset(X_full_train[dates_full_train < cur_block_num], label=y_full_train[dates_full_train < cur_block_num]), 100)\n",
    "    pred_lgb_level2 = lgb.predict(X_full_train[dates_full_train == cur_block_num].values)\n",
    "    \n",
    "    X_train_level2[dates_train_level2.isin([cur_block_num]), 0] = pred_lr_level2.copy()\n",
    "    X_train_level2[dates_train_level2.isin([cur_block_num]), 1] = pred_lgb_level2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train_level2[:,0], X_train_level2[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_plus_level2 = np.c_[X_full_train[dates_train.isin(level2_date_blocks)].values, X_train_level2]\n",
    "\n",
    "X_train_plus_level2 = np.c_[X_full_train[dates_full_train.isin(level2_date_blocks)].values, X_train_level2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_plus_level2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# meta_model = lightgbm.train(lgb_params, lightgbm.Dataset(X_train_plus_level2, label=y_train_level2), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_model = RandomForestRegressor(max_depth=5)\n",
    "\n",
    "meta_model.fit(X_train_plus_level2, y_train_level2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = LinearRegression()\n",
    "meta_model.fit(X_train_plus_level2, y_train_level2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_model = XGBRegressor(max_depth=5,learning_rate=0.03,n_jobs=-1,random_state=seed)\n",
    "meta_model.fit(X_train_plus_level2, y_train_level2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_plus = np.c_[X_test.values, X_test_level2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # valid_preds = meta_model.predict(valid_plus)\n",
    "# rmse_valid_stacking = rmse(y_valid, valid_preds)\n",
    "\n",
    "print('Train rmse for stacking is %f' % rmse_train_stacking)\n",
    "# print('Test  rmse for stacking is %f' % rmse_valid_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_pred = meta_model.predict(X_test_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell automatically submits the submission file to kaggle. However, it should be carefully executed because the submitting opportunities are limited.\n",
    "- remove '#' before submitting\n",
    "- add a meaningful message to a submission\n",
    "- my rule is using a filename for models and a message for data\n",
    "    - ex) filename: rf_n20m13.csv, message: ot-sic-sm-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c competitive-data-science-final-project -f ./submission/rf_n20_m13.csv -m \"ot-sic-sm-20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c competitive-data-science-final-project -f ./submission/xgb_m10_eta003.csv -m \"ot-sic-sm-20\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check public score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c competitive-data-science-final-project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
